@inproceedings{Attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{Bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
}

@InProceedings{Fine-Tune,
author="Sun, Chi
and Qiu, Xipeng
and Xu, Yige
and Huang, Xuanjing",
editor="Sun, Maosong
and Huang, Xuanjing
and Ji, Heng
and Liu, Zhiyuan
and Liu, Yang",
title="How to Fine-Tune BERT for Text Classification?",
booktitle="Chinese Computational Linguistics",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="194--206",
abstract="Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, BERT (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets.",
isbn="978-3-030-32381-3"
}
@inproceedings{
cruz2023reinforcement,
title={Reinforcement Learning Fine-tuning of Language Models is Biased Towards More Extractable Features},
author={Diogo Cruz and Edoardo Pona and Alex Holness-Tofts and Elias Schmied and V{'\i}ctor Abia Alonso and Charlie Griffin and Bogdan-Ionut Cirstea},
booktitle={Socially Responsible Language Modelling Research},
year={2023},
url={https://openreview.net/forum?id=ee0kxTFS9a}
}

@InProceedings{Arabert,
author="Touahri, Ibtissam",
editor="Lazaar, Mohamed
and En-Naimi, El Mokhtar
and Zouhair, Abdelhamid
and Al Achhab, Mohammed
and Mahboub, Oussama",
title="AraBERT with GANs for High Performance Fine-Grained Dialect Classification",
booktitle="Proceedings of the 6th International Conference on Big Data and Internet of Things",
year="2023",
publisher="Springer International Publishing",
address="Cham",
pages="160--170",
abstract="Arabic nations share the same mother tongue, Arabic. However the used vernacular language is different, that, in turn, may vary from one region to another. In this paper, we aim to identify various dialects by performing text classification. We distinguish between Moroccan, Algerian, Tunisian, Egyptian, and Lebanese Arabic dialects denoted respectively MDA, ADA, TDA, EDA and LDA. Aside from explaining the collecting process of system resources, we identify linguistic specificities that characterize each dialect. We build models from the preprocessed text using a combination of the pretrained model, AraBERT; and Generative Adversarial Networks (GANS). The work establishes the foundation of a dialect identification system by gathering freely available corpora and reaching the state-of-the-art.",
isbn="978-3-031-28387-1"
}
@inproceedings{chowdhury2020improving,
  title={Improving Arabic Text Categorization Using Transformer Training Diversification},
  author={Chowdhury, Shammur Absar and Abdelali, Ahmed and Darwish, Kareem and Soon-Gyo, Jung and Salminen, Joni and Jansen, Bernard J},
  booktitle={Proceedings of the Fifth Arabic Natural Language Processing Workshop},
  pages={226--236},
  year={2020}
}

@Inbook{Rus2013,
author="Rus, Vasile",
editor="Runehov, Anne L. C.
and Oviedo, Lluis",
title="Natural Language Processing",
bookTitle="Encyclopedia of Sciences and Religions",
year="2013",
publisher="Springer Netherlands",
address="Dordrecht",
pages="1401--1404",
isbn="978-1-4020-8265-8",
doi="10.1007/978-1-4020-8265-8_1225",
url="https://doi.org/10.1007/978-1-4020-8265-8_1225"
}

