@inproceedings{Attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{Bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
}

@InProceedings{Fine-Tune,
author="Sun, Chi
and Qiu, Xipeng
and Xu, Yige
and Huang, Xuanjing",
editor="Sun, Maosong
and Huang, Xuanjing
and Ji, Heng
and Liu, Zhiyuan
and Liu, Yang",
title="How to Fine-Tune BERT for Text Classification?",
booktitle="Chinese Computational Linguistics",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="194--206",
abstract="Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, BERT (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets.",
isbn="978-3-030-32381-3"
}
@inproceedings{
cruz2023reinforcement,
title={Reinforcement Learning Fine-tuning of Language Models is Biased Towards More Extractable Features},
author={Diogo Cruz and Edoardo Pona and Alex Holness-Tofts and Elias Schmied and V{'\i}ctor Abia Alonso and Charlie Griffin and Bogdan-Ionut Cirstea},
booktitle={Socially Responsible Language Modelling Research},
year={2023},
url={https://openreview.net/forum?id=ee0kxTFS9a}
}

@InProceedings{Arabert,
author="Touahri, Ibtissam",
editor="Lazaar, Mohamed
and En-Naimi, El Mokhtar
and Zouhair, Abdelhamid
and Al Achhab, Mohammed
and Mahboub, Oussama",
title="AraBERT with GANs for High Performance Fine-Grained Dialect Classification",
booktitle="Proceedings of the 6th International Conference on Big Data and Internet of Things",
year="2023",
publisher="Springer International Publishing",
address="Cham",
pages="160--170",
abstract="Arabic nations share the same mother tongue, Arabic. However the used vernacular language is different, that, in turn, may vary from one region to another. In this paper, we aim to identify various dialects by performing text classification. We distinguish between Moroccan, Algerian, Tunisian, Egyptian, and Lebanese Arabic dialects denoted respectively MDA, ADA, TDA, EDA and LDA. Aside from explaining the collecting process of system resources, we identify linguistic specificities that characterize each dialect. We build models from the preprocessed text using a combination of the pretrained model, AraBERT; and Generative Adversarial Networks (GANS). The work establishes the foundation of a dialect identification system by gathering freely available corpora and reaching the state-of-the-art.",
isbn="978-3-031-28387-1"
}
@inproceedings{chowdhury2020improving,
  title={Improving Arabic Text Categorization Using Transformer Training Diversification},
  author={Chowdhury, Shammur Absar and Abdelali, Ahmed and Darwish, Kareem and Soon-Gyo, Jung and Salminen, Joni and Jansen, Bernard J},
  booktitle={Proceedings of the Fifth Arabic Natural Language Processing Workshop},
  pages={226--236},
  year={2020}
}

@Inbook{Rus2013,
author="Rus, Vasile",
editor="Runehov, Anne L. C.
and Oviedo, Lluis",
title="Natural Language Processing",
bookTitle="Encyclopedia of Sciences and Religions",
year="2013",
publisher="Springer Netherlands",
address="Dordrecht",
pages="1401--1404",
isbn="978-1-4020-8265-8",
doi="10.1007/978-1-4020-8265-8_1225",
url="https://doi.org/10.1007/978-1-4020-8265-8_1225"
}


@article{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{antoun2020arabert,
  title={AraBERT: Transformer-based Model for Arabic Language Understanding},
  author={Antoun, Wissam and Baly, Fady and Hajj, Hazem},
  journal={arXiv preprint arXiv:2003.00104},
  year={2020}
}

@article{acegpt,
  title={ACE GPT: Advanced Cognitive Engine for Generative Pre-trained Transformer},
  author={OpenAI},
  journal={OpenAI Technical Report},
  year={2023}
}

@article{acegptchat,
  title={ACE GPT Chat: Enhancing Conversational AI with Advanced Cognitive Engine},
  author={OpenAI},
  journal={OpenAI Technical Report},
  year={2023}
}

@article{nlpmentalhealth,
  title={Natural Language Processing in Mental Health Applications Using Chatbots: A Systematic Review},
  author={Miner, Adam S and Milstein, Amy and Schueller, Stephen and Hegde, Ritika and Mangurian, Christina and Linos, Eleni},
  journal={Journal of Medical Internet Research},
  volume={18},
  number={11},
  pages={e240},
  year={2016},
  publisher={JMIR Publications}
}

@article{sentimentanalysis,
  title={Sentiment Analysis for Mental Health: Detecting and Interpreting Emotions in Text Data},
  author={Mohammad, Saif M},
  journal={IEEE Intelligent Systems},
  volume={29},
  number={3},
  pages={38--43},
  year={2014},
  publisher={IEEE}
}

@article{aiacceptance,
  title={User Acceptance of Artificial Intelligence-Driven Mental Health Interventions: A Mixed-Methods Study},
  author={Laranjo, Liliana and Dunn, Adam G and Tong, Huong L and Kocaballi, Ahmet B and Chen, Jiayun and Bashir, Ruba and Surian, Didi and Gallego, Blanca and Magrabi, Farah and Lau, Annie YS and others},
  journal={JMIR mental health},
  volume={7},
  number={11},
  pages={e17750},
  year={2020},
  publisher={JMIR Publications}
}

@misc{betterhelp,
  title={Online Counseling \& Therapy},
  author={BetterHelp},
  note={Retrieved from \url{https://www.betterhelp.com/}}
}

@misc{talkspace,
  title={Online Therapy \& Counseling via Messaging, Video \& Phone},
  author={Talkspace},
  note={Retrieved from \url{https://www.talkspace.com/}}
}

@article{fitzpatrick2017woebot,
  title={Delivering cognitive behavior therapy to young adults with symptoms of depression and anxiety using a fully automated conversational agent (Woebot): A randomized controlled trial},
  author={Fitzpatrick, Kathleen K. and Darcy, Alison and Vierhile, Molly},
  journal={JMIR Mental Health},
  volume={4},
  number={2},
  pages={e19},
  year={2017},
  doi={10.2196/mental.7785}
}

@article{inkster2018wysa,
  title={An empathy-driven, conversational artificial intelligence agent (WYSA) for digital mental well-being: Real-world data evaluation mixed-methods study},
  author={Inkster, Becky and Sarda, Siddharth and Subramanian, Vishalakshi},
  journal={JMIR mHealth and uHealth},
  volume={6},
  number={11},
  pages={e12106},
  year={2018},
  doi={10.2196/12106}
}

@inproceedings{de2014mental,
  title={Mental health discourse on reddit: Self-disclosure, social support, and anonymity},
  author={De Choudhury, Munmun and De, Shubham},
  booktitle={Proceedings of the International AAAI Conference on Web and Social Media},
  volume={8},
  number={1},
  year={2014},
  note={Retrieved from \url{https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewFile/8075/8082}}
}

@article{calvo2017sentiment,
  title={Natural language processing in mental health applications using non-clinical texts},
  author={Calvo, Rafael A. and Milne, David N. and Hussain, Mubbasir S. and Christensen, Helen},
  journal={Natural Language Engineering},
  volume={23},
  number={5},
  pages={649-685},
  year={2017},
  doi={10.1017/S1351324916000383}
}

@article{bickmore2010user,
  title={Maintaining reality: Relational agents for antipsychotic medication adherence},
  author={Bickmore, Timothy W. and Puskar, Kathryn and Schlenk, Elizabeth A. and Pfeifer, Lori M. and Sereika, Susan M.},
  journal={Interacting with Computers},
  volume={22},
  number={4},
  pages={276-288},
  year={2010},
  doi={10.1016/j.intcom.2010.02.002}
}

@inproceedings{abadi2016tensorflow,
  title={TensorFlow: A system for large-scale machine learning},
  author={Abadi, Mart√≠n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Kudlur, Manjunath},
  booktitle={12th {USENIX} symposium on operating systems design and implementation ({OSDI} 16)},
  pages={265-283},
  year={2016},
  note={Retrieved from \url{https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf}}
}

@inproceedings{paszke2019pytorch,
  title={PyTorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Chintala, Soumith},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8024-8035},
  year={2019},
  note={Retrieved from \url{https://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}}
}

@article{mittelstadt2016ethics,
  title={The ethics of algorithms: Mapping the debate},
  author={Mittelstadt, Brent Daniel and Allo, Patrick and Taddeo, Mariarosaria and Wachter, Sandra and Floridi, Luciano},
  journal={Big Data \& Society},
  volume={3},
  number={2},
  pages={2053951716679679},
  year={2016},
  doi={10.1177/2053951716679679}
}
